# -*- coding: utf-8 -*-
"""pre-processing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dgNx_9-fib8fRgC8owQvlbwP8S3fn_a2

# Tugas Pemrosesan Bahasa Alami


*   Tujuan: mencari kesamaan antara 2 abstrak
*   Anggota Kelompok
    *   Muhammad Kurnia Sani (200411100046)
    *   Desi Rahmawati (220411100003)
    *   Olifia Nuris Saffana (220411100024)

## Impor _library_
"""

import string
import re
import numpy as np

import nltk
from nltk.tokenize import word_tokenize
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet as wn
from nltk.tokenize import RegexpTokenizer

!pip install Sastrawi
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('averaged_perceptron_tagger')
nltk.download('wordnet')

from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer
import pandas as pd

# unsupervised learning ANN
from sklearn.neural_network import BernoulliRBM

# normalization
from sklearn.preprocessing import MinMaxScaler
MinMaxscaler = MinMaxScaler()
from sklearn.preprocessing import MaxAbsScaler
MABSscaler = MaxAbsScaler()

# preprocessing
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer

# similarity
from sklearn.metrics.pairwise import cosine_similarity

"""## Data _toy_"""

corpus_1 = [
    '"sistem  tersebut  juga  berpotensi  mengahadapi  kendala-kendala  yaitu  dalam  proses  transaksi  data  karena  padatnya jaringan yang menuju database SIAKAD"',
    '"Web crawler aplikasi yang berfungsi mengambil informasi yang diperlukan dengan cara menjelajahi halaman situs blog kemudian, disimpan dalam database"',
    '"web crawler akan berjalan menelusuri halaman web dan mengumpulkan data-data kemudian, melakukan proses download data yang telah dikumpulkan oleh web crawler"',
    'Hasil uji coba sistem yang terbaik untuk deteksi Corepoint sidik jari menggunakan metode Geometry Of Region Technique (GR) dengan M0=50 dan V0=50 pada proses normalisasi',
    '"Sistem pencarian yang dipakai saat ini adalah pencarian database dimana masih belum mampu memberikan hasil pencarian yang sesuai dengan konteks atau keywords yang diinputkan"',
    'Web crawler akan berjalan melalui halaman web dan mengumpulkan informasi kemudian melakukan proses unduh data yang telah dikumpulkan oleh web crawler'
]

corpus_2 = [
    "Peran Pihak Internal yaitu untuk menjalankan perusahaan dengan memanfaatkan sumber daya yang ada secara efektif. Peran Pihak Eksternal yaitu untuk mendukung keberlangsungan usaha."
    "Lingkungan internal bekerja dengan baik dalam lingkungan bisnis dalam lingkungan tersebut external bekerja di lingkungan tidak resmi yang bertugas dalam memximalkan bisnis",
    "Lingkungan Internal -> Tenaga kerja,modal dan peralatan. Peran dari lingkungan Internal ini untuk menentukan kekuatan perusahaan dan juga mengetahui kelemahan perusahaan. Lingkungan Eksternal -> Konsumen,Pesaing, Ekonomi. Peran dari lingkungan Eksternal ini untuk menentukan peluang pasar dan juga ancaman dari luar/pesaing",
    "Lingkungan Internal (Dalam) contohnya karyawan, manager, CEO dan lain-lain yang memiliki peran langsung (direct) untuk memperngaruhi nasib perusahaan Lingkungan Eksternal (Luar) contohnya Konsumen, pesaing, pemasok dan lain-lain yang memiliki peran bahwa kekuatan lingkungan luar tersebut memiliki potensi mempengaruhi kinerja atau keberlangsungan perusahaan.",
]

corpus_3 = [
    '"sistem  tersebut  juga  berpotensi  mengahadapi  kendala-kendala"',
    '"Web crawler aplikasi yang berfungsi mengambil informasi"',
    '"web crawler akan berjalan menelusuri halaman web"',
    'Hasil uji coba sistem yang terbaik untuk deteksi Corepoint sidik jari"',
    '"Sistem pencarian yang dipakai saat ini adalah pencarian database"',
]

"""## Preprocessing

### _stemming_, _lemmatizing_, dan _remove stopwords_
"""

# create stemmer
factory = StemmerFactory()
sastrawi_stemmer = factory.create_stemmer()

# lemmatizer
nltk_lemmatizer = WordNetLemmatizer()

# stopword removal
stopwords = nltk.corpus.stopwords.words('indonesian')

"""### Fungsi untuk _preprocessing_ data"""

def remove_digits(word):
    pattern = '[0-9]'
    return re.sub(pattern, '', word)

def remove_punctuation(word_token):
    return word_token if word_token not in string.punctuation else ""

def tokenize_and_lowercase(doc):
    return [word.lower() for sent in nltk.sent_tokenize(doc) for word in nltk.word_tokenize(sent)]

def stemmer(word_token):
    return sastrawi_stemmer.stem(word_token)

def lemmatizer(stemmed_word):
    return nltk_lemmatizer.lemmatize(stemmed_word)

def finishing(tokens_result):
    return list(filter(lambda token: token.isalpha() or token.isdigit() or token.isspace(), tokens_result))

def stringify(token_data):
    return  ' '.join(token_data)

def preprocessing(corpus_data):
    str_data = []
    token_data = []

    for idx, doc in enumerate(corpus_data):
        print(f'dokumen ke-{idx}')

        tokens_result = []

        # process 1 & 2
        lowercased_tokens = tokenize_and_lowercase(doc)
        # print(lowercased_tokens)

        for word_token in lowercased_tokens:

            # process 3
            if word_token not in stopwords:
                # process 4 & 5
                stem = stemmer(remove_punctuation(remove_digits(word_token)))

                # process 6
                lem = lemmatizer(stem)

                tokens_result.append(lem)

        tokenized_data = finishing(tokens_result)

        str_data.append(stringify(tokenized_data))
        token_data.append(tokenized_data)

    return {
        "str_token": str_data,
        "tokens": token_data,
    }

preprocessed_corpus = preprocessing(corpus_3)

preprocessed_corpus["tokens"]

preprocessed_corpus["str_token"]

"""### Term Frequency"""

def create_TF_df(term_freqs):
    return pd.DataFrame(data = term_freqs['df'].toarray(), columns = term_freqs['features'])

"""# RBM

### Function Definition

#### to calc euclidean
"""

import math
def euclidean_distance(v1, v2):
  """
  Calculates the Euclidean distance between two vectors.

  Args:
    v1: First vector (list of values).
    v2: Second vector (list of values with same length as v1).

  Returns:
    The Euclidean distance between v1 and v2.
  """
  distance_squared = 0
  for i in range(len(v1)):
    distance_squared += (v1[i] - v2[i])**2
  return math.sqrt(distance_squared)

def calc_eu_dist(hidden_representations):
    distances = {}
    for i in range(len(hidden_representations)):
        row = []
        for j in range(len(hidden_representations)):
            if i != j:  # Avoid calculating distance to itself (always 0)
                distance = euclidean_distance(hidden_representations[i], hidden_representations[j])
                row.append(distance)
            else:
                row.append(0)  # Distance to itself is 0
        distances[f'abs {i}'] = row
    return distances

def get_max_eu_dist(distances):
    max_distance = 0
    for row in distances.values():
        for distance in row:
            max_distance = max(max_distance, distance)
    return max_distance

def normalize_distance(distance, max_distance):
  """
  Normalizes a distance value between 0 and 1.

  Args:
    distance: The Euclidean distance to normalize.
    max_distance: The maximum distance observed in the distance matrix.

  Returns:
    The normalized distance between 0 (most similar) and 1 (least similar).
  """
  return ( distance / max_distance ) * 100

def calc_sim_eu(distances, max_distance):
    distances_norm = {}
    for key in distances.keys():
        row = []
        for distance in distances[key]:
            if distance != 0:  # Avoid calculating distance to itself (always 0)
                distance = normalize_distance(distance, max_distance)
                row.append(distance)
            else:
                row.append(0)  # Distance to itself is 0
            distances_norm[key] = row
    return pd.DataFrame(distances_norm, index=docs_li)

"""## Data Preparation"""

data_url = 'https://raw.githubusercontent.com/MuhammadKurniaSani-me/data_tugas_akhir_UTM/main/Data_TA.csv'

data = pd.read_csv(data_url)

data.columns

data.isnull().sum()

data = data.dropna().reset_index(drop=True)

data.isnull().values.any()

df = data['Abstrak']

df

preprocessed_df = preprocessing(df[:5])

"""## Hitung _term frequency_"""

count_vectorizer = CountVectorizer(binary=True)

"""### Toy

"""

preprocessed_df["str_token"]

X_toy = count_vectorizer.fit_transform(preprocessed_corpus["str_token"])
y_toy = count_vectorizer.get_feature_names_out()

df_toy = create_TF_df({'df':X_toy, 'features':y_toy})

df_toy

"""### Real"""

X = count_vectorizer.fit_transform(preprocessed_df['str_token'])

y = count_vectorizer.get_feature_names_out()

tf_real = create_TF_df({'df':X, 'features':y})

tf_real

"""## Modelling"""

# Train BernoulliRBM
rbm_toy = BernoulliRBM(n_components=4, n_iter=10)

"""### Data _toy_"""

abstracts_transformed_toy = rbm_toy.fit_transform(X_toy)

docs_li = [f'abs {doc_idx + 1}' for doc_idx in range(len(abstracts_transformed_toy))]

abstracts_transformed_toy

"""### real data"""

# Train BernoulliRBM
rbm = BernoulliRBM(n_components=4, n_iter=10)

abstracts_transformed_real = rbm.fit_transform(X)

docs_li_r = [f'abs {doc_idx + 1}' for doc_idx in range(len(abstracts_transformed_real))]

abstracts_transformed_real

docs_li_r

"""## Hitung kesamaan

### Toy

#### cosine
"""

# Calculate cosine similarities
co_sim = cosine_similarity(abstracts_transformed_toy)

# Print similarity matrix
pd.DataFrame(co_sim, columns=docs_li, index=docs_li)

"""#### euclidean"""

eu_dist_toy = calc_eu_dist(abstracts_transformed_toy)
max_eu_dist_toy = get_max_eu_dist(eu_dist_toy)
eu_sim_toy = calc_sim_eu(eu_dist_toy, max_eu_dist_toy)

eu_sim_toy

"""### Real

#### cosine
"""

# Calculate cosine similarities
co_sim_r = cosine_similarity(abstracts_transformed_real)

# Print similarity matrix
pd.DataFrame(co_sim_r, columns=docs_li_r, index=docs_li_r)

"""#### euclidean"""

eu_dist_real = calc_eu_dist(abstracts_transformed_real)
max_eu_dist_real = get_max_eu_dist(eu_dist_real)
eu_sim_real = calc_sim_eu(eu_dist_real, max_eu_dist_real)

eu_sim_real

"""## evaluasi"""

from sklearn.metrics import mean_absolute_percentage_error

"""### Real

#### abs1
"""

docs_li_r

y_actual = {
    'abs 1':[0, 45.1, 39.6, 44.5, 44.1],
    'abs 2':[45.1, 0, 38.1, 37.9, 43.6],
    'abs 3':[39.6, 38.1, 0, 34.0, 39.7],
    'abs 4':[44.5, 37.9, 34.8, 0, 42.1],
    'abs 5':[44.1, 43.6, 39.7, 42.1, 0],
}

pd.DataFrame(y_actual, index=docs_li_r)

np.array(list(y_actual.values()))[:2, :2]

eu_sim_real.values[:2,:2]

val_e = mean_absolute_percentage_error(np.array(list(y_actual.values()))[:2,:2], eu_sim_real.values[:2,:2])
print(f'persentase kesalahan model = {round(val_e * 100, 2)}%')

val_e = mean_absolute_percentage_error(np.array(list(y_actual.values()))[:,:], eu_sim_real.values[:,:])
print(f'persentase kesalahan model = {round(val_e * 100, 2)}%')